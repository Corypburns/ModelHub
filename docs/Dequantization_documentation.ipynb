{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c53a50f",
   "metadata": {},
   "source": [
    "##### Dequantization: What is it?\n",
    "\n",
    "Dequantization is the process of converting already quantized values back to their floating-point values after the model makes its predictions on what it is working with. Starting with floating-point values, these values are more accurate and precise, while uint values are slightly less accurate. There's a caveat, however. Floating-point values take up more memory than uint values on account of being more precise. In the process of dequantization, the model makes its predictions with the uint values and converts them back to floating-point. This acts as a way to save memory, while also getting accurate results as the results will all be accurate (or as accurate as they can be) at the end of each inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1b2814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Dequantization Formula example (These are all example values)\n",
    "output_data = np.array([130, 128, 140], dtype=np.uint8)\n",
    "scalar = 0.00390625\n",
    "zero_point = 128\n",
    "\n",
    "final_float_values = (output_data - zero_point) * scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ce6208",
   "metadata": {},
   "source": [
    "A softmax is applied to these output values which has a maximum of 1.0, or 100% after converting the softmax values to a percentage by multiplying by 100; this is how the dequantization to confidence value process works. Below is an example of how to use numpy to get the softmax of an array of raw output data.\n",
    "\n",
    "**Note**:  These values should sum up to about 1.0, or as stated above, 100 when converted to percentages. If it isn't 1, then it should be VERY close to it otherwise the data isn't accurate and something is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0bb30ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.65900114 0.24243297 0.09856589] 1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Softmax\n",
    "def softmax(raw_data):\n",
    "    logits = np.exp(raw_data - np.max(raw_data))\n",
    "    return logits / logits.sum()\n",
    "\n",
    "\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "probability = softmax(logits)\n",
    "print(probability, probability.sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
